"""
M√£ h√≥a t·∫≠p d·ªØ li·ªáu Iris v√† l∆∞u th√†nh file m·ªõi
L∆∞u public key v√† secret key ri√™ng bi·ªát
"""

import tenseal as ts
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pickle
import json
import os
from datetime import datetime

print("="*80)
print(" M√É H√ìA T·∫¨P D·ªÆ LI·ªÜU IRIS V·ªöI FHE")
print(" L∆∞u d·ªØ li·ªáu m√£ h√≥a + Public Key + Secret Key")
print("="*80)

# T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ
output_dir = 'f:/FHE/encrypted_data'
os.makedirs(output_dir, exist_ok=True)

# ============================================================================
# B∆∞·ªõc 1: Load v√† chu·∫©n b·ªã d·ªØ li·ªáu
# ============================================================================
print("\n[1] Loading Iris dataset from local file...")

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file iris.data
iris_file = 'F:/FHE/iris/iris.data'
print(f"  - Reading from: {iris_file}")

# ƒê·ªçc file CSV
df = pd.read_csv(iris_file, header=None, 
                 names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])

# T√°ch features v√† labels
X = df.iloc[:, :4].values  # 4 features ƒë·∫ßu
y_labels = df.iloc[:, 4].values  # Class labels

# Convert class labels th√†nh s·ªë
class_mapping = {
    'Iris-setosa': 0,
    'Iris-versicolor': 1,
    'Iris-virginica': 2
}
y = np.array([class_mapping[label] for label in y_labels])

print(f"‚úì Data loaded from local file:")
print(f"  - Total samples: {len(X)}")
print(f"  - Features: {X.shape[1]}")
print(f"  - Classes: {len(np.unique(y))}")

# Chia train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Chu·∫©n h√≥a
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚úì Data split:")
print(f"  - Training: {len(X_train)} samples")
print(f"  - Testing: {len(X_test)} samples")

# L∆∞u metadata
feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']

metadata = {
    'dataset': 'Iris',
    'data_source': iris_file,
    'n_samples_train': len(X_train),
    'n_samples_test': len(X_test),
    'n_features': X_train.shape[1],
    'n_classes': len(np.unique(y)),
    'feature_names': feature_names,
    'target_names': target_names,
    'encrypted_at': datetime.now().isoformat(),
    'encryption_scheme': 'CKKS',
    'poly_modulus_degree': 8192,
    'scale': 2**40
}

# ============================================================================
# B∆∞·ªõc 2: T·∫°o FHE Context v√† Keys
# ============================================================================
print("\n[2] Creating FHE context and keys...")

# T·∫°o context
context = ts.context(
    ts.SCHEME_TYPE.CKKS,
    poly_modulus_degree=8192,
    coeff_mod_bit_sizes=[60, 40, 40, 60]
)
context.global_scale = 2**40
context.generate_galois_keys()

print("‚úì FHE context created")
print(f"  - Scheme: CKKS")
print(f"  - Polynomial modulus: 8192")
print(f"  - Security level: ~128-bit")

# ============================================================================
# B∆∞·ªõc 3: L∆∞u Keys
# ============================================================================
print("\n[3] Saving keys...")

# L∆∞u SECRET KEY (context ƒë·∫ßy ƒë·ªß v·ªõi secret key)
secret_context_bytes = context.serialize(save_secret_key=True)
with open(f'{output_dir}/secret_key.bin', 'wb') as f:
    f.write(secret_context_bytes)

secret_key_size = len(secret_context_bytes)
print(f"‚úì Secret key saved: secret_key.bin ({secret_key_size/1024:.2f} KB)")
print(f"  ‚ö†Ô∏è  QUAN TR·ªåNG: Gi·ªØ b√≠ m·∫≠t file n√†y!")

# L∆∞u PUBLIC KEY (context kh√¥ng c√≥ secret key)
public_context_bytes = context.serialize(save_secret_key=False)
with open(f'{output_dir}/public_key.bin', 'wb') as f:
    f.write(public_context_bytes)

public_key_size = len(public_context_bytes)
print(f"‚úì Public key saved: public_key.bin ({public_key_size/1024:.2f} KB)")
print(f"  ‚ÑπÔ∏è  File n√†y c√≥ th·ªÉ chia s·∫ª c√¥ng khai")

# ============================================================================
# B∆∞·ªõc 4: M√£ h√≥a d·ªØ li·ªáu
# ============================================================================
print("\n[4] Encrypting data...")

def encrypt_matrix(context, matrix):
    """M√£ h√≥a ma tr·∫≠n"""
    encrypted_rows = []
    for i, row in enumerate(matrix):
        encrypted_rows.append(ts.ckks_vector(context, row.tolist()))
        if (i + 1) % 20 == 0:
            print(f"  - Encrypted {i+1}/{len(matrix)} samples...")
    return encrypted_rows

# M√£ h√≥a training data
print("Encrypting training features...")
encrypted_X_train = encrypt_matrix(context, X_train_scaled)

print("Encrypting training labels...")
encrypted_y_train = []
for label in y_train:
    encrypted_y_train.append(ts.ckks_vector(context, [float(label)]))

# M√£ h√≥a test data
print("Encrypting test features...")
encrypted_X_test = encrypt_matrix(context, X_test_scaled)

print("Encrypting test labels...")
encrypted_y_test = []
for label in y_test:
    encrypted_y_test.append(ts.ckks_vector(context, [float(label)]))

print("‚úì All data encrypted")

# ============================================================================
# B∆∞·ªõc 5: L∆∞u d·ªØ li·ªáu m√£ h√≥a
# ============================================================================
print("\n[5] Saving encrypted data...")

# Serialize encrypted data
def serialize_encrypted_list(encrypted_list):
    """Convert list of encrypted vectors to bytes"""
    return [enc.serialize() for enc in encrypted_list]

# Training data
train_data = {
    'features': serialize_encrypted_list(encrypted_X_train),
    'labels': serialize_encrypted_list(encrypted_y_train)
}

with open(f'{output_dir}/encrypted_train.pkl', 'wb') as f:
    pickle.dump(train_data, f)

train_size = os.path.getsize(f'{output_dir}/encrypted_train.pkl')
print(f"‚úì Encrypted training data saved: encrypted_train.pkl ({train_size/1024/1024:.2f} MB)")

# Test data
test_data = {
    'features': serialize_encrypted_list(encrypted_X_test),
    'labels': serialize_encrypted_list(encrypted_y_test)
}

with open(f'{output_dir}/encrypted_test.pkl', 'wb') as f:
    pickle.dump(test_data, f)

test_size = os.path.getsize(f'{output_dir}/encrypted_test.pkl')
print(f"‚úì Encrypted test data saved: encrypted_test.pkl ({test_size/1024/1024:.2f} MB)")

# ============================================================================
# B∆∞·ªõc 6: L∆∞u d·ªØ li·ªáu g·ªëc (ƒë·ªÉ so s√°nh)
# ============================================================================
print("\n[6] Saving original data for comparison...")

original_data = {
    'X_train': X_train_scaled,
    'y_train': y_train,
    'X_test': X_test_scaled,
    'y_test': y_test,
    'scaler': scaler
}

with open(f'{output_dir}/original_data.pkl', 'wb') as f:
    pickle.dump(original_data, f)

original_size = os.path.getsize(f'{output_dir}/original_data.pkl')
print(f"‚úì Original data saved: original_data.pkl ({original_size/1024:.2f} KB)")

# ============================================================================
# B∆∞·ªõc 7: L∆∞u metadata
# ============================================================================
print("\n[7] Saving metadata...")

metadata['file_sizes'] = {
    'secret_key_bytes': secret_key_size,
    'public_key_bytes': public_key_size,
    'encrypted_train_bytes': train_size,
    'encrypted_test_bytes': test_size,
    'original_data_bytes': original_size,
    'size_overhead': (train_size + test_size) / original_size
}

with open(f'{output_dir}/metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print(f"‚úì Metadata saved: metadata.json")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "="*80)
print(" HO√ÄN TH√ÄNH M√É H√ìA D·ªÆ LI·ªÜU")
print("="*80)

total_encrypted_size = train_size + test_size
total_original_size = X_train_scaled.nbytes + y_train.nbytes + X_test_scaled.nbytes + y_test.nbytes

print(f"""
üìÅ FILES CREATED:
   {output_dir}/
   ‚îú‚îÄ‚îÄ secret_key.bin          ({secret_key_size/1024:.2f} KB) üîí B√ç M·∫¨T
   ‚îú‚îÄ‚îÄ public_key.bin          ({public_key_size/1024:.2f} KB) üîì C√¥ng khai
   ‚îú‚îÄ‚îÄ encrypted_train.pkl     ({train_size/1024/1024:.2f} MB) üîê D·ªØ li·ªáu m√£ h√≥a
   ‚îú‚îÄ‚îÄ encrypted_test.pkl      ({test_size/1024/1024:.2f} MB) üîê D·ªØ li·ªáu m√£ h√≥a
   ‚îú‚îÄ‚îÄ original_data.pkl       ({original_size/1024:.2f} KB) üìä D·ªØ li·ªáu g·ªëc
   ‚îî‚îÄ‚îÄ metadata.json           Th√¥ng tin chi ti·∫øt

üìä SIZE COMPARISON:
   ‚Ä¢ Original data:    {total_original_size/1024:.2f} KB
   ‚Ä¢ Encrypted data:   {total_encrypted_size/1024/1024:.2f} MB
   ‚Ä¢ Size overhead:    {total_encrypted_size/total_original_size:.1f}x

üîê SECURITY:
   ‚Ä¢ Encryption: CKKS (Fully Homomorphic)
   ‚Ä¢ Security level: ~128-bit
   ‚Ä¢ Can compute on encrypted data
   ‚Ä¢ Only secret key holder can decrypt

‚ö†Ô∏è  IMPORTANT:
   ‚Ä¢ secret_key.bin: GI·ªÆ B√ç M·∫¨T - D√πng ƒë·ªÉ decrypt k·∫øt qu·∫£
   ‚Ä¢ public_key.bin: C√≥ th·ªÉ chia s·∫ª - D√πng ƒë·ªÉ encrypt d·ªØ li·ªáu m·ªõi
   ‚Ä¢ encrypted_*.pkl: C√≥ th·ªÉ g·ª≠i cho server ƒë·ªÉ t√≠nh to√°n

üí° NEXT STEPS:
   1. Gi·ªØ secret_key.bin an to√†n
   2. C√≥ th·ªÉ g·ª≠i public_key.bin + encrypted data cho server
   3. Server t√≠nh to√°n tr√™n d·ªØ li·ªáu m√£ h√≥a
   4. Client d√πng secret_key.bin ƒë·ªÉ decrypt k·∫øt qu·∫£
""")

print("="*80)
print("‚úÖ DONE!")
print("="*80)
