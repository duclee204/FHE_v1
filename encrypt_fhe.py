"""encrypt_fhe.py
Script m√£ h√≥a c√°c file `iris_train.csv` v√† `iris_test.csv` b·∫±ng TenSEAL (CKKS).
- Normalize d·ªØ li·ªáu tr∆∞·ªõc khi m√£ h√≥a (StandardScaler)
- T·∫°o context CKKS, t·∫°o key pair (public/secret) v√† l∆∞u v√†o `keys/`.
- M√£ h√≥a c·∫£ features (4 s·ªë th·ª±c) v√† labels (map th√†nh integers) cho m·ªói m·∫´u.
- L∆∞u ciphertexts d∆∞·ªõi d·∫°ng bytes v√†o `encrypted_iris/`.
- L∆∞u scaler ƒë·ªÉ c√≥ th·ªÉ denormalize sau n√†y.

Y√™u c·∫ßu: pip install -r requirements.txt
"""
import os
import pickle
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

try:
    import tenseal as ts
except Exception as e:
    raise SystemExit("TenSEAL kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: python -m pip install -r requirements.txt")

KEYS_DIR = 'keys'
IRIS_DIR = 'iris'
ENCRYPTED_DIR = 'encrypted_iris'  # Th∆∞ m·ª•c ri√™ng cho d·ªØ li·ªáu m√£ h√≥a
TRAIN_CSV = os.path.join(IRIS_DIR, 'iris_train.csv')
TEST_CSV  = os.path.join(IRIS_DIR, 'iris_test.csv')

os.makedirs(KEYS_DIR, exist_ok=True)
os.makedirs(ENCRYPTED_DIR, exist_ok=True)

# Mapping cho l·ªõp
LABEL_MAP = {
    'Iris-setosa': 0,
    'Iris-versicolor': 1,
    'Iris-virginica': 2
}

# ƒê·ªçc CSV
train_df = pd.read_csv(TRAIN_CSV)
test_df  = pd.read_csv(TEST_CSV)

features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

print(f"Train samples: {len(train_df)} | Test samples: {len(test_df)}")

# ============================================================================
# NORMALIZE D·ªÆ LI·ªÜU (Quan tr·ªçng cho FHE training!)
# ============================================================================
print("\n[Data Normalization]")
print("Normalizing features using StandardScaler...")

# T√°ch features v√† labels
X_train = train_df[features].values
y_train = train_df['class'].map(LABEL_MAP).values

X_test = test_df[features].values
y_test = test_df['class'].map(LABEL_MAP).values

# Fit scaler tr√™n train data
scaler = StandardScaler()
X_train_normalized = scaler.fit_transform(X_train)
X_test_normalized = scaler.transform(X_test)

print(f"‚úì Features normalized")
print(f"  Original range: [{X_train.min():.2f}, {X_train.max():.2f}]")
print(f"  Normalized range: [{X_train_normalized.min():.2f}, {X_train_normalized.max():.2f}]")
print(f"  Mean: {X_train_normalized.mean():.4f}, Std: {X_train_normalized.std():.4f}")

# L∆∞u scaler ƒë·ªÉ denormalize sau n√†y
scaler_path = os.path.join(ENCRYPTED_DIR, 'scaler.pkl')
with open(scaler_path, 'wb') as f:
    pickle.dump(scaler, f)
print(f"‚úì Scaler saved to {scaler_path}")

print(f"\n[Key Generation]")

print(f"\n[Key Generation]")

# T·∫°o TenSEAL context (CKKS) v√† keypair
print("T·∫°o context CKKS v√† sinh keypair (c√≥ th·ªÉ m·∫•t v√†i gi√¢y)...")
# poly_modulus_degree=8192 l√† ƒë·ªß cho b·∫£o m·∫≠t 128-bit v√† hi·ªáu nƒÉng t·ªët
context = ts.context(
    ts.SCHEME_TYPE.CKKS,
    poly_modulus_degree=8192,
    coeff_mod_bit_sizes=[60, 40, 40, 60]
)
context.global_scale = 2**40
context.generate_galois_keys()  # c·∫ßn cho rotation n·∫øu d√πng

# L∆∞u context (ch·ª©a c·∫£ public key)
ctx_bytes = context.serialize(save_secret_key=False)
sec_bytes = context.serialize(save_secret_key=True)  # ch·ª©a c·∫£ secret key

with open(os.path.join(KEYS_DIR, 'tenseal_context_public.bin'), 'wb') as f:
    f.write(ctx_bytes)
with open(os.path.join(KEYS_DIR, 'tenseal_context_secret.bin'), 'wb') as f:
    f.write(sec_bytes)

print(f"‚úì Keys saved to {KEYS_DIR}/")

# ============================================================================
# H√†m m√£ h√≥a d·ªØ li·ªáu ƒë√£ normalize
# ============================================================================
print(f"\n[Encryption]")
print("Encrypting normalized data...")

def encrypt_normalized_data(X_normalized, y_labels, out_path, dataset_name):
    """M√£ h√≥a d·ªØ li·ªáu ƒë√£ normalize"""
    samples = []
    labels = []
    
    n_samples = len(X_normalized)
    
    for i in range(n_samples):
        # M√£ h√≥a features (ƒë√£ normalize)
        vals = X_normalized[i].tolist()
        ctxt = ts.ckks_vector(context, vals)
        samples.append(ctxt.serialize())

        # M√£ h√≥a nh√£n
        lbl = float(y_labels[i])
        lbl_ctxt = ts.ckks_vector(context, [lbl])
        labels.append(lbl_ctxt.serialize())
        
        if (i + 1) % 20 == 0 or i == n_samples - 1:
            print(f"  {dataset_name}: {i+1}/{n_samples} encrypted...")

    payload = {
        'n_features': X_normalized.shape[1],
        'samples': samples,   # list of bytes
        'labels': labels,
        'metadata': {
            'n_samples': len(samples),
            'is_normalized': True,
            'normalization_method': 'StandardScaler'
        }
    }

    with open(out_path, 'wb') as f:
        pickle.dump(payload, f)
    print(f"‚úì {dataset_name} encrypted -> {out_path}")
    
    return payload

# M√£ h√≥a v√† l∆∞u
train_out = os.path.join(ENCRYPTED_DIR, 'iris_train_ctxts.pkl')
test_out  = os.path.join(ENCRYPTED_DIR, 'iris_test_ctxts.pkl')

encrypt_normalized_data(X_train_normalized, y_train, train_out, "Train")
encrypt_normalized_data(X_test_normalized, y_test, test_out, "Test")

print(f'\n{"="*70}')
print('‚úÖ Ho√†n th√†nh m√£ h√≥a!')
print(f'{"="*70}')
print(f'üìÅ V·ªã tr√≠ files:')
print(f'  - Keys: {KEYS_DIR}/')
print(f'  - Encrypted data: {ENCRYPTED_DIR}/')
print(f'  - Scaler: {ENCRYPTED_DIR}/scaler.pkl')
print(f'\nüí° Ghi ch√∫:')
print(f'  - D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c NORMALIZE tr∆∞·ªõc khi m√£ h√≥a')
print(f'  - Normalized range: [{X_train_normalized.min():.2f}, {X_train_normalized.max():.2f}]')
print(f'  - S·ª≠ d·ª•ng ts.context_from() v√† ts.ckks_vector_from() ƒë·ªÉ t·∫£i l·∫°i')
print(f'{"="*70}')
